services:
  vllm:
    image: vllm/vllm-openai:v0.9.2
    container_name: vllm-server
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    runtime: nvidia
    command:
      - "--model"
      - "meta-llama/Llama-3.2-3B-Instruct"
      - "--gpu-memory-utilization"
      - "0.9"
      - "--max-model-len"
      - "54301"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 10m
      timeout: 2s
      retries: 5
      start_period: 3m   

  dcm-api:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    container_name: dcm-api
    depends_on:
      vllm:
        condition: service_healthy


    ports:
      - "8080:8080"
    environment:
      - VLLM_SERVER=http://vllm:8000/v1
      - QDRANT_URL=http://qdrant:6333
    volumes:
      - ./app/data:/src/app/data
    env_file:
      - .env

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"     # REST API